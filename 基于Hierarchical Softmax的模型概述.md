1.word2vec介绍

word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。
虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的word2vec代码为准。本文关注于word2vec的基础知识。

2.word2vec基础之霍夫曼树

word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。
具体如何用霍夫曼树来进行CBOW和Skip-Gram的训练我们在下一节讲，这里我们先复习下霍夫曼树。
霍夫曼树的建立其实并不难，过程如下：
输入：权值为(w1,w2,...wn)的n个节点
输出：对应的霍夫曼树
1）将(w1,w2,...wn)看做是有n棵树的森林，每个树仅有一个节点。
2）在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。
3） 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。
4）重复步骤2）和3）直到森林里只有一棵树为止。
首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是20,8,6,16,7。此时根节点权重最小的6,7合并，得到新子树，
依次类推，最终得到下面的霍夫曼树。

![1](https://github.com/wonderfultina/word2vec/blob/master/images/51C6.tmp.jpg)



那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，
这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？
一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。


3.基于Hierarchical Softmax的模型概述

传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。
这个模型如下图所示。其中V是词汇表的大小。

![1](https://github.com/wonderfultina/word2vec/blob/master/images/59D9.tmp.png)


word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。
比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。

第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。
如何映射呢？这里就是理解word2vec的关键所在了。
由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。
我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词w2。

和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,
其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。
在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为"Hierarchical Softmax"。
如何“沿着霍夫曼树一步步完成”呢？
在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：

![1](https://github.com/wonderfultina/word2vec/blob/master/images/62E7.tmp.png)


其中xw是当前内部节点的词向量，而θ则是我们需要从训练样本求出的逻辑回归的模型参数。
使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为V,现在变成了log2V。
第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。
容易理解，被划分为左子树而成为负类的概率为P(−)=1−P(+)。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看P(−),P(+)谁的概率值大。
而控制P(−),P(+)谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数θ。
对于上图中的w2，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点n(w2,1)的P(−)概率大，n(w2,2)的P(−)概率大，n(w2,3)的P(+)概率大。
回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点θ, 使训练样本达到最大似然。那么如何达到最大似然呢？


4. 基于Hierarchical Softmax的模型梯度计算

我们使用最大似然法来寻找所有节点的词向量和所有内部节点θ。先拿上面的w2例子来看，我们期望最大化下面的似然函数：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/6BAB.tmp.png)

对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。
为了便于我们后面一般化的描述，我们定义输入的词为w,其从输入层词向量求和平均后的霍夫曼树根节点词向量为xw, 从根节点到w所在的叶子节点，包含的节点总数为lw, w在霍夫曼树中从根节点开始，经过的第i个节点表示为pwi,对应的霍夫曼编码为dwi∈{0,1},其中i=2,3,...lw。而该节点对应的模型参数表示为θwi, 其中i=1,2,...lw−1，没有i=lw是因为模型参数仅仅针对于霍夫曼树的内部节点。

定义w经过的霍夫曼树某一个节点j的逻辑回归概率为P(dwj|xw,θwj−1)，其表达式为：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/5.png)

那么对于某一个目标输出词w,其最大似然为：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/6.png)

在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到w的对数似然函数L如下：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/7.png)


要得到模型中w词向量和内部节点的模型参数θ, 我们使用梯度上升法即可。首先我们求模型参数θwj−1的梯度：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/8.png)

同样的方法，可以求出xw的梯度表达式如下：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/9.png)


5. 基于Hierarchical Softmax的CBOW模型

首先我们要定义词向量的维度大小M，以及CBOW的上下文大小2c,这样我们对于训练样本中的每一个词，其前面的c个词和后面的c个词作为了CBOW模型的输入,该词本身作为样本的输出，期望softmax概率最大。
在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。
对于从输入层到隐藏层（投影层），这一步比较简单，就是对w周围的2c个词向量求和取平均即可，即：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/10.png)

第二步，通过梯度上升法来更新我们的θwj−1和xw，注意这里的xw是由2c个词向量相加而成，我们做梯度更新完毕后会用梯度项直接更新原始的各个xi(i=1,2,,,,2c)，即：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/11.png)


其中η为梯度上升法的步长。
这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法：
输入：基于CBOW的语料训练样本，词向量的维度大小M，CBOW的上下文大小2c,步长η
输出：霍夫曼树的内部节点模型参数θ，所有的词向量w
1. 基于语料训练样本建立霍夫曼树。
2. 随机初始化所有的模型参数θ，所有的词向量w
3. 进行梯度上升迭代过程，对于训练集中的每一个样本(context(w),w)做如下处理：

![4](https://github.com/wonderfultina/word2vec/blob/master/images/12.png)

